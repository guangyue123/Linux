# A模块题目：OpenStack 平台部署与运维 

业务场景： 

某企业拟使用OpenStack搭建一个企业云平台，用于部署各类企业应用对外对内服务。云平台可实现I

T资源池化、弹性分配、集中管理、性能优化以及统一安全认证等。系统结构如下图：

 

企业云平台的搭建使用竞赛平台提供的两台云服务器，配置如下表：

 

| 设备名称  | 主机名                    | 接口 | IP地址                                   |
| --------- | ------------------------- | ---- | ---------------------------------------- |
| 云服务器1 | controller                | eth0 | 公网IP:******** 私网IP:192.168.100.*/24  |
| eth1      | 私网IP:192.168.200.  */24 |      |                                          |
| 云服务器2 | compute                   | eth0 | 公网IP:******** 私网IP:192.168.100. */24 |
| eth1      | 私网IP:192.168.200.  */24 |      |                                          |

 

 

说明： 

1.选手自行检查工位pc机硬件及网络是否正常； 

2.竞赛使用集群模式进行，给每个参赛队提供华为云账号和密码及考试系统的账号和密码。选手通过用户名与密码分别登录华为云和考试系统；

3.竞赛用到的软件包都在云主机/root下。 

4.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。 

​    

任务 **1** 私有云平台环境初始化（**5** 分） 

# 1. 配置主机名 

  把controller节点主机名设置为controller, compute节点主机名设置为compute，修改 hosts文件将IP地址映射为主机名。  在controller节点将cat /etc/hosts命令的返回结果提交到答题框。【1分】 

````
vi /etc/hosts
```
<controller ip> controller
<compute ip> compute
```
````



# 2. 挂载光盘镜像 

在controller节点的root目录下有CentOS-7-x86_64-DVD-2009.iso和openstack-trai n.tar.gz，在/opt下创建centos目录，将镜像文件CentOS-7-x86_64-DVD-2009.iso挂载到/ opt/centos下，将openstack-train.tar.gz解压到/opt目录下，并创建本地yum源local.repo。 

在controller节点将yum list | grep glance命令的返回结果提交到答题框。【1分】

````
# 在/opt下创建centos目录
mkdir /opt/centos
# 将镜像文件临时挂载到/mnt目录下
mount -o loop CentOS-7-x86_64-DVD-2009.iso
# 复制/mnt目录下的文件到/opt/centos
cp -rfv /mnt/* /opt/centos
# 取消挂载CentOS-7-x86_64-DVD-2009.iso
umount /mnt
# 将openstack-train.tar.gz解压到/opt目录下
tar -zxvf openstack-train.tar.gz -C /opt
# 创建本地yum源local.repo
vi /etc/yum.repos.d/local.repo
```
[centos]
name=centos
baseurl=file:///opt/centos
gpgcheck=0
enabled=1
[iaas]
name=iaas
baseurl=file:///opt/iaas/iaas-repo
gpgcheck=0
enabled=1
```
````



  

# 3. 搭建ftp服务器 

在controller节点上安装vsftp服务,将/opt目录设为共享，并设置为开机自启动，然

后重启服务生效；在compute节点创建FTP源ftp.repo，使用controller节点为FTP服务器，配置文件中的FTP地址使用主机名。 

在compute节点将cat /etc/yum.repos.d/ftp.repo命令的返回结果提交到答题框。【1

分】 

````
# 在controller节点上安装vsftp服务
yum insatll -y vsftpd
# 将/opt目录设为共享
vi /etc/vsftpd/vsftpd.conf
```
anon_root=/opt
```
# 设置为开机自启动，然后重启服务生效
systemctl enable vsftpd && systemctl restart vsftpd
# 在compute节点创建FTP源ftp.repo，使用controller节点为FTP服务器，配置文件中的FTP地址使用主机名。 
vi /etc/yum.repos.d/ftp.repo
```
[centos]
name=centos
baseurl=ftp://controller/centos
gpgcheck=0
enabled=1
[iaas]
name=iaas
baseurl=ftp://controller/iaas/iaas-repo
gpgcheck=0
enabled=1
```
````





# 4. 分区 

在compute节点将sdb分为两个区分别为sdb1和sdb2,大小自定义。要求分区格式为gp

t,使用mkfs.xfs命令对文件系统格式化。将lsblk -f命令的返回结果提交到答题框。【1分】 

```
gdisk /dev/sdb
mkfs.xfs /dev/sdb1
mkfs.xfs /dev/sdb2
```



# 5. 系统调优-脏数据回写 

Linux系统内存中会存在脏数据，一般系统默认脏数据占用内存30%时会回写磁盘，修改系统配置文件，要求将回写磁盘的大小调整为60%。 

在controller节点将sysctl -p命令的返回结果提交到答题框。【1分】 

```
要将 Linux 系统默认脏数据回写磁盘的阈值从 30% 调整为 60%，你需要修改系统的参数配置文件。在大多数 Linux 发行版中，这个配置文件是 “/etc/sysctl.conf”。

下面是一般的修改步骤：

打开终端或命令行窗口，以管理员权限登录系统。
执行以下命令以编辑 “/etc/sysctl.conf” 配置文件：
sudo nano /etc/sysctl.conf
如果你喜欢其他编辑器，也可以将 “nano” 替换为你喜欢的编辑器的命令。

在配置文件中，找到或添加以下行：
vm.dirty_ratio = 60
这将设置脏数据回写磁盘的阈值为 60%。

保存文件并退出编辑器。

执行以下命令以使系统加载新的配置：

sudo sysctl -p
这将重新加载系统配置文件，并应用新的脏数据回写磁盘的阈值。

现在，系统的脏数据回写磁盘的阈值应该已经调整为 60%。请注意，这是一个全局的系统参数设置，可能会影响整个系统中的所有进程。确保在进行这样的修改之前对系统做好适当的备份，并了解修改明确的后果和风险。
```



任务 **2 OpenStack** 搭建任务（**10** 分） 

# 1. 修改脚本文件 

在controller节点和compute节点分别安装sh-guoji软件包，修改脚本文件基本变量（脚本文件为/root/variable.sh），修改完成后使用命令生效该脚本文件并替换到compute节点对应位置。

在controller节点请将echo $HOST_NAME $HOST_NAME_NODE命令的返回结果提交到答题

框。【1分】 

````
vi /root/variable.sh
```
#--------------------system Config--------------------##
#Controller Server Manager IP. example:x.x.x.x
HOST_IP=<controller ip>

#Controller HOST Password. example:000000 
HOST_PASS=000000

#Controller Server hostname. example:controller
HOST_NAME=controller

#Compute Node Manager IP. example:x.x.x.x
HOST_IP_NODE=<compute ip>

#Compute HOST Password. example:000000 
HOST_PASS_NODE=000000

#Compute Node hostname. example:compute
HOST_NAME_NODE=compute

#--------------------Chrony Config-------------------##
#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)
network_segment_IP=<x.x.0.0/16>

#--------------------Rabbit Config ------------------##
#user for rabbit. example:openstack
RABBIT_USER=openstack

#Password for rabbit user .example:000000
RABBIT_PASS=000000

#--------------------MySQL Config---------------------##
#Password for MySQL root user . exmaple:000000
DB_PASS=000000

#--------------------Keystone Config------------------##
#Password for Keystore admin user. exmaple:000000
DOMAIN_NAME=demo
ADMIN_PASS=000000
DEMO_PASS=000000

#Password for Mysql keystore user. exmaple:000000
KEYSTONE_DBPASS=000000

#--------------------Glance Config--------------------##
#Password for Mysql glance user. exmaple:000000
GLANCE_DBPASS=000000

#Password for Keystore glance user. exmaple:000000
GLANCE_PASS=000000

#--------------------Nova Config----------------------##
#Password for Mysql nova user. exmaple:000000
NOVA_DBPASS=000000

#Password for Keystore nova user. exmaple:000000
NOVA_PASS=000000

#--------------------Neturon Config-------------------##
#Password for Mysql neutron user. exmaple:000000
NEUTRON_DBPASS=000000

#Password for Keystore neutron user. exmaple:000000
NEUTRON_PASS=000000

#metadata secret for neutron. exmaple:000000
METADATA_SECRET=000000

#Tunnel Network Interface. example:x.x.x.x
INTERFACE_IP=<localhost ip>

#External Network Interface. example:eth1
INTERFACE_NAME=eth1

#External Network The Physical Adapter. example:provider
Physical_NAME=provider

#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101
minvlan=1

#Last Vlan ID in VLAN RANGE for VLAN Network. example:200
maxvlan=4000

#--------------------Cinder Config--------------------##
#Password for Mysql cinder user. exmaple:000000
CINDER_DBPASS=000000

#Password for Keystore cinder user. exmaple:000000
CINDER_PASS=000000

#Cinder Block Disk. example:md126p3
BLOCK_DISK=sdb1

#--------------------Swift Config---------------------##
#Password for Keystore swift user. exmaple:000000
SWIFT_PASS=000000

#The NODE Object Disk for Swift. example:md126p4.
OBJECT_DISK=sdb2

#The NODE IP for Swift Storage Network. example:x.x.x.x.
STORAGE_LOCAL_NET_IP=<compute ip>

#--------------------Heat Config----------------------##
#Password for Mysql heat user. exmaple:000000
HEAT_DBPASS=000000

#Password for Keystore heat user. exmaple:000000
HEAT_PASS=000000

#--------------------Zun Config-----------------------##
#Password for Mysql Zun user. exmaple:000000
ZUN_DBPASS=000000

#Password for Keystore Zun user. exmaple:000000
ZUN_PASS=000000

#Password for Mysql Kuryr user. exmaple:000000
KURYR_DBPASS=000000

#Password for Keystore Kuryr user. exmaple:000000
KURYR_PASS=000000

#--------------------Ceilometer Config----------------##
#Password for Gnocchi ceilometer user. exmaple:000000
CEILOMETER_DBPASS=000000

#Password for Keystore ceilometer user. exmaple:000000
CEILOMETER_PASS=000000

#--------------------AODH Config----------------##
#Password for Mysql AODH user. exmaple:000000
AODH_DBPASS=000000

#Password for Keystore AODH user. exmaple:000000
AODH_PASS=000000

#--------------------Barbican Config----------------##
#Password for Mysql Barbican user. exmaple:000000
BARBICAN_DBPASS=000000

#Password for Keystore Barbican user. exmaple:000000
BARBICAN_PASS=000000

```
````





# 2. 安装openstack基础组件 

分别在controller节点和compute节点执行openstack-completion.sh文件(执行完闭需重连终端)。在controller节点将openstack --version命令的返回结果提交到答题框。【1分】

# 3. 搭建数据库组件 

在controller节点执行openstack-controller-mysql.sh脚本，会自行安装mariadb、m

emcached、rabbitmq等服务和完成相关配置。执行完成后修改配置文件将缓存CACHESIZE修改为128,并重启相应服务。 

将ps aux|grep memcached命令的返回结果提交到答题框。【1分】

```
修改配置文件：
找到 Memcached 配置文件和 RabbitMQ 配置文件，然后修改缓存大小 (CACHESIZE)。通常情况下，Memcached 的配置文件位于 /etc/memcached.conf，RabbitMQ 的配置文件位于 /etc/rabbitmq/rabbitmq.conf。

使用文本编辑器（如 nano 或 vim）打开这些文件，找到相关的配置项并将缓存大小修改为 128。例如，在 Memcached 配置文件中，你可能会找到类似这样的行：
-cachesize 64
将其修改为：
-cachesize 128
在 RabbitMQ 配置文件中，你可能会找到类似这样的行：
vm_memory_high_watermark.relative = 0.4
将其修改为：
vm_memory_high_watermark.relative = 0.2
重启服务：
保存配置文件修改后，重新启动 Memcached 和 RabbitMQ 服务，使其应用新的配置：
sudo systemctl restart memcached
sudo systemctl restart rabbitmq-server
这样，修改的配置就会生效，并且 Memcached 和 RabbitMQ 会使用新的缓存大小。
```

 

# 4. 搭建认证服务组件 

在controller节点执行openstack-controller-keystone.sh脚本，会自行安装keystone服务和完成相关配置。使用openstack命令，创建一个名为tom的账户，密码为tompasswo rd123,邮箱为tom@example.com。 

将openstack user show tom命令的返回结果提交到答题框。【1分】 

```
openstack user create --domain demo --password tompassword123 --email tom@example.com tom
```



# 5. 搭建镜像服务组件 

在controller节点执行openstack-controller-glance.sh脚本，会自行安装glance服

务和完成相关配置。完成后使用openstack命令,创建一个qcow2格式，名为cirros_0.3.4 的镜像，镜像文件使用cirros-0.3.4-x86_64-disk.img。 

将openstack image show cirros_0.3.4命令的返回结果提交到答题框。【1分】 

```
glance image-create --name "cirros" --disk-format qcow2 --min-disk 10 --container-format bare --progress < ./cirros-0.3.4-x86_64-disk.img
```



# 6. 搭建计算服务组件 

在controller节点执行openstack-controller-nova.sh，compute节点执行openstackcompute-nova.sh，会自行安装nova服务和完成相关配置。使用openstack命令创建一个名为 m1,ID为56，内存为2048MB,磁盘容量为20GB,vCPU数量为2的云主机类型。在controller节点将openstack flavor show m1命令的返回结果提交到答题框。【1分】 

```
# 使用openstack命令创建一个名为 m1,ID为56，内存为2048MB,磁盘容量为20GB,vCPU数量为2的云主机类型
openstack flavor create --id 56 --ram 2048 --disk 20 --vcpus 2 m1
```



# 7. 搭建网络组件并初始化网络 

在controller节点执行openstack-controller-neutron.sh,compute节点执行openstac k-compute-neutron.sh，会自行安装neutron服务并完成配置。创建云主机外部网络ext-net，子网为ext-subnet，云主机浮动 IP 可用网段为 192.168.200.100~192.168.200.200，网关为 192.168.200.1。在controller节点将openstack subnet show ext-subnet命令的返回结果提交到答题框。

【2分】 

```
openstack network create --external --provider-network-type flat --provider-physical-network ext-net ext-net
openstack subnet create --network ext-net --allocation-pool start=192.168.200.100,end=192.168.200.200 --subnet-range 192.168.200.0/24 --gateway 192.168.200.1 ext-subnet

```



# 8. 搭建图形化界面 

在 controller节点执行openstack-controller-dashboard.sh脚本，会自行安装 dashboard服务并完成配置。请修改compute节点nova配置文件，使之后创建的实例可以在网页通过公网访问控制台页面。

在compute节点请将cat /etc/nova/nova.conf | grep 公网IP 命令的返回结果提交到

答题框。（例:cat /etc/nova/nova.conf | grep 121.36.12.138）【2分】 

````
vi /etc/nova/nova.conf
```
添加以下配置项，以允许实例的 VNC 控制台通过公网访问。确保将 <controller_public_IP> 替换为实际的 Controller 节点的公网 IP 地址。
[vnc]
novncproxy_base_url = http://<controller_public_IP>:6080/vnc_auto.html
xvpvncproxy_base_url = http://<controller_public_IP>:6081/console
novncproxy_port = 6080
xvpvncproxy_port = 6081
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = <compute_node_IP>
```
# 重启compute节点nova服务
systemctl restart nova-compute
````



任务 **3 OpenStack** 运维任务（**20** 分） 

某公司构建了一套内部私有云系统，这套私有云系统将为公司内部提供计算服务。你将作为该私有云的维护人员，请完成以下运维工作。

# 1. 数据库管理 

请使用数据库命令将所有数据库备份到/root路径下,备份文件名为openstack.sql，完成

后使用命令查看文件属性其中文件大小以mb显示。请将所有命令和返回结果提交到答题框。【1分】 

```
# 使用数据库命令将所有数据库备份到/root路径下,备份文件名为openstack.sql
mysqldump --all-databases > /root/openstack.sql
# 使用命令查看文件属性其中文件大小以mb显示
ls -lh /root/openstack.sql
```



# 2. 数据库管理 

进入数据库，创建本地用户examuser，密码为 000000，然后查询mysql数据库中的 use

r 表的user,host,password字段。然后赋予这个用户所有数据库的“查询”“删除”“更新” “创建”的权限。 

将select User, Select_priv,Update_priv,Delete_priv,Create_priv from user;命令的返回结果提交到答题框。【1分】 

```
# 进入数据库
mysql -uroot -p000000
# 创建本地用户examuser，密码为 000000
CREATE USER 'examuser'@'localhost' IDENTIFIED BY '000000';
# 赋予这个用户所有数据库的“查询”“删除”“更新” “创建”的权限
GRANT SELECT, UPDATE, DELETE, CREATE ON *.* TO 'examuser'@'localhost';
```



# 3. 安全组管理 

使用openstack命令创建名称为group_web的安全组该安全组的描述为”Custom securi

ty group”，用openstack命令为安全组添加icmp规则和ssh规则允许任意ip地址访问web, 完成后使用openstack命令查看该安全组的详细信息。 

将openstack security group show group_web命令的返回结果提交到答题框。【2分】 

```
# 使用openstack命令创建名称为group_web的安全组该安全组的描述为”Custom security group”
openstack security group create group_web --description "Custom security group"
# 允许 ICMP 规则：
openstack security group rule create --proto icmp group_web
# 允许 SSH 规则：
openstack security group rule create --proto tcp --dst-port 22 group_web
```



# 4. 项目管理 

在 keystone 中使用openstack创建shop项目添加描述为”Hello shop”，完成后使用openstack命令禁用该项目，然后使用openstack命令查看该项目的详细信息。将openstack project show shop命令的返回结果提交到答题框。【2分】

```
# 在keystone 中使用openstack创建shop项目添加描述为”Hello shop”
openstack project create --description "Hello shop" shop
```



# 5. 用户管理 

使用openstack命令查看admin租户的当前配额值、将admin租户的实例配额提升到13，

然后查看修改后admin租户的配额值。

将openstack quota show admin命令的返回结果提交到答题框。【2分】 

```
# 使用openstack命令查看admin租户的当前配额值
openstack quota show --project admin
# 将admin租户的实例配额提升到13
openstack quota set --instances 13 admin
```



# 6. heat模板管理 

执行脚本openstack-controller-heat.sh安装完heat服务后，编写Heat模板create_f

lavor.yaml，创建名为“m2.flavor”、ID为 1234、内存为1024MB、硬盘为20GB、vcpu数量为1的云主机类型，创建完成后使用openstack命令查看堆栈列表。将openstack stack list命令的返回结果提交到答题框。【2分】 、

````
# 编写Heat模板create_flavor.yaml
vi create_flavor.yaml
```
heat_template_version: 2013-05-23
resources:
  m2_flavor:
    type: OS::Nova::Flavor
    properties:
      name: m2.flavor
      id: 1234
      ram: 1024
      vcpus: 1
      disk: 20
```
# 使用Heat创建堆栈
openstack stack create -t create_flavor.yaml m2-flavor-stack
````



# 7. 后端配置文件管理 

修改glance后端配置文件，将项目的映像存储限制为10GB,完成后重启glance服务。 

将cat /etc/glance/glance-api.conf |grep _quota命令的返回结果提交到答题框。【2 分】 

````
vi /etc/glance/glance-api.conf
```
_image_property_quota = 10737418240 # # 10GB in bytes
```
# 重启glance服务
systemctl restart glance-api
systemctl restart glance-registry
````



# 8. 存储服务管理 

在controller节点执行openstack-controller-cinder.sh,compute节点执行openstack -compute-cinder.sh，在controller和compute节点上会自行安装cinder服务并完成配置。使用openstack命令创建一个名为lvm的卷类型，使用cinder命令创建该类型规格键值对，要求lvm卷类型对应cinder后端驱动lvm所管理的存储资源,名字lvm_test，大小1G的云硬盘并查询该云硬盘的详细信息。

将cinder show lvm_test命令的返回结果提交到答题框。【2分】 

```
# 创建名为lvm的卷类型
openstack volume type create lvm
# 使用cinder命令创建规格键值对
cinder type-key lvm set volume_backend_name=lvm
# 创建1G大小的云硬盘
openstack volume create --size 1 --type lvm lvm_test
# 查询云硬盘的详细信息
openstack volume show lvm_test
```



# 9. 存储管理 

为了减缓来自实例的数据访问速度的变慢，OpenStack Block Storage 支持对卷数据复制带宽的速率限制。请修改cinder后端配置文件将卷复制带宽限制为最高100MiB/s（对应数值修改为104857600）。 

将cat /etc/cinder/cinder.conf | grep 104857600命令的返回结果提交到答题框。【2 分】 

````
vi /etc/cinder/cinder.conf
```
[DEFAULT]
replication_bandwidth = 104857600
```
# 重启cinder服务
systemctl restart openstack-cinder-*
````



# 10. 存储管理 

在controller节点执行openstack-controller-swift.sh,compute节点执行openstack-

compute-swift.sh，在controller和compute节点上会自行安装swift服务并完成配置。使用swift命令创建一个名为file的容器并查看，然后把cirros-0.3.4-x86_64-disk.img上传到file容器中。将swift stat file命令的返回结果提交到答题框。【2分】 

```
# 用swift命令创建一个名为file的容器
swift post file
# 把cirros-0.3.4-x86_64-disk.img上传到file容器中
swift upload file cirros-0.3.4-x86_64-disk.img
# 查看刚创建的swift容器
swift stat file
```



# 11. OpenStack API 管理 

使用curl的方式获取admin用户token值；使用已获取的token值通过curl的方式获取

domain为default所有用户名（ip使用主机名）。将获取到的所有用户名提交到答题框。【2分】

```

```

任务四 **OpenStack** 架构任务（**15** 分） 

# 1. 安装python3环境 

在controller节点安装python3环境。安装完之后查看python3版本，使用提供的whl文件安装依赖。将pip3 list命令的返回结果提交到答题框。【5分】 

```
# 安装python
yum install -y epel-release
yum install -y python3 python3-pip
# 使用提供的 whl 文件安装依赖
pip3 install example.whl
# 查看已安装的 Python 包列表
pip3 list
```



# 2. python对接OpenStack API创建image镜像 

编写python代码对接OpenStack API，完成镜像的上传。在controller节点的/root目录下创建create_image.py文件，在该文件中编写python代码对接openstack api（需在py 文件中获取token），要求在openstack私有云平台中上传镜像cirros-0.3.4-x86_64-disk.i mg，名字为cirros001，disk_format为qcow2，container_format为bare。执行完代码要求输出“创建镜像成功，id为：xxxxxx”。 

分别将cat /root/create_image.py命令和python3 create_image.py命令的返回结果提交到答题框。【5分】 

# 3. python对接OpenStack API创建用户 

编写python代码对接OpenStack API，完成用户的创建。在controller节点的/root目

录下创建create_user.py文件，在该文件中编写python代码对接openstack api（需在py 文件中获取token），要求在openstack私有云平台中创建用户guojibeisheng。 

将cat /root/create_user.py命令的返回结果提交到答题框。【5分】 



 



B模块题目：容器的编排与运维 

 

某企业计划使用 k8s 平台搭建微服务系统，现在先使用简单的微服务项目进行测试，请按照要求完成相应任务。 

表 1 IP 地址规划 

| 设备名称  | 主机名 | 接口 | IP地址                                   | 说明                     |
| --------- | ------ | ---- | ---------------------------------------- | ------------------------ |
| 云服务器1 | master | eth0 | 公网 IP:******** 私网IP:192.168.100.*/24 | Harbor也是使用该云服务器 |
| 云服务器2 | node   | eth0 | 公网 IP:******** 私网IP:192.168.100.*/24 |                          |

说明： 

1.表1中的公网IP和私网IP以自己云主机显示为准，每个人的公网IP和私网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。 

2.华为云中云主机名字已命好，直接使用对应名字的云主机即可。

  3.所有的软件包存放于/root目录。

​    

任务 1容器云平台环境初始化（10.5分） 

# 1.容器云平台的初始化 

master 节点主机名设置为 master、node 节点主机名设置为 node，所有节点 root 密码设置为 000000,所有

节点关闭 swap，并配置 hosts 映射。

请在 master 节点将 ping node -c 3 命令的返回结果提交到答题框。【1.5 分】

````
# 在Master节点上执行以下命令以设置主机名为"master"：
hostnamectl set-hostname master
# 在Node节点上执行以下命令以设置主机名为"node"：
hostnamectl set-hostnmae node
# 所有节点 root 密码设置为 000000
sudo passwd root
# 关闭Swap：
swapoff -a
# 在系统重启后保持Swap被禁用，您需要编辑 /etc/fstab 文件并注释掉包含Swap的行
# 配置hosts映射
vi /etc/hosts
```
<master ip> master
<node ip> node
```
#在Master节点上执行以下命令以测试与Node节点的连接：
ping node -c 3
````



# 2．镜像文件的复制 

/root 目录存放有 CentOS-7-x86_64-DVD-2009.iso 和 kubernetes_V1.2.iso 光盘镜像文件，在/opt 目录下使用命令创建 centos 目录，并将镜像文件 CentOS-7-x86_64-DVD-2009.iso 中的内容复制到 centos 目录下，将镜像文件 kubernetes_V1.2.iso 中的内容复制到 /opt 目录下。请在 master 节点将 du -h /opt/ --max-depth=1 命令的返回结果提交到答题框。【1.5 分】 

```
# 创建 centos 目录
mkdir /opt/centos
# 将镜像文件 CentOS-7-x86_64-DVD-2009.iso 中的内容复制到 centos 目录下
mount -o loop CentOS-7-x86_64-DVD-2009.iso /mnt
cp -rfv /mnt/* /opt/centos
umount /mnt
# 复制kubernetes镜像内容
mount -o loop kubernetes_V1.2.iso /mnt
cp -rfv /mnt/* /opt
umount /mnt
```



# 3．Yum源的编写

在 master 节点首先将系统自带的 yum 源移动到/home 目录，然后为 master 节点配置本地 yum 源，yum 源文件名为 local.repo。

将 yum repolist 命令的返回结果提交到答题框。【1.5 分】 

````
#将系统自带的 yum 源移动到/home 目录
mv /etc/yum.repos.d/* /home
#为 master 节点配置本地 yum 源，yum 源文件名为 local.repo
vi /etc/yum.repos.d/local.repo
```
[centos]
name=centos
baseurl=file:///opt/centos
gpgcheck=0
enabled=1
[kubernetes]
name=kubernetes
baseurl=file:///opt/kubernetes-repo
gpgcheck=0
enabled=1
```
````



# 4．安装ftp服务

在 master 节点安装 ftp 服务，将 ftp 共享目录设置为 /opt。

将 ps -ef | grep ftp 命令的返回结果提交到答题框。【1.5 分】 

````
# 安装ftp服务
yum install -y vsftpd
# 将ftp共享目录设置为/opt
vi /etc/vsftpd/vsftpd.conf
```
anon_root=/opt
```
# 重启ftp服务
systemctl enable vsftpd && systemctl restart vsftpd
````



# 5．ftp源的编写 

为 node 节点配置 ftp 源，ftp 源文件名称为 ftp.repo，其中 ftp 服务器地址为 master 节点,配置 ftp 源时不

要写 IP 地址。 

在 node 节点请将 curl ftp://master 命令的返回结果提交到答题框。【1.5 分】 

````
# 为 node 节点配置 ftp 源，ftp 源文件名称为 ftp.repo
mv /etc/yum.repos.d/* /home
vi /etc/yum.repos.d/ftp.repo
```
[centos]
name=centos
baseurl=ftp://master/centos
gpgcheck=0
enabled=1
[kubernetes]
name=kubernetes
baseurl=ftp://master/kubernetes-repo
gpgcheck=0
enabled=1
```
````



# 6.设置时间同步服务器 

在 master 节点上部署 chrony 服务器，允许其它节点同步时间，启动服务并设置为开机自启动；在其他节点上指定 master 节点为上游 NTP 服务器，重启服务并设为开机自启动。（配置文件 IP 用计算机名代替）在 node 节点将 chronyc sources 命令的返回结果提交到答题框。【1.5 分】 

````
# 在Master节点上执行以下步骤：
# 安装Chrony服务：
yum install -y chrony
# 编辑Chrony配置文件：
vi /etc/chrony/chrony.conf
```
allow <node ip>
```
# 启动Chrony服务并设置为开机自启动：
systemctl start chronyd
systemctl enable chronyd
# 在node上执行以下步骤：
yum install -y chrony
# 编辑Chrony配置文件：
vi /etc/chrony/chrony.conf
```
server <master ip> iburst
```
# 重启Chrony服务并设置为开机自启动：
systemctl restart chronyd
systemctl enable chronyd
# 查看Chrony的时间同步源：
chronyc sources
````



# 7.设置免密登录 

为两台台服务器设置免密登录，保证服务器之间能够互相免密登录。

在 master 节点将 ssh node 命令的返回结果提交到答题框。【1.5 分】 

```
# 在本地主机上创建公钥和密钥
ssh-keygen
# 用 ssh-copy-id 把公钥复制到远程主机
 ssh-copy-id root@<远程主机 ip>
# 直接登录远程主机
ssh <远程主机 ip>
```



任务2 k8s搭建任务（19.5分） 

# 1.安装docker应用

在所有节点上安装 dokcer-ce,并设置为开机自启动。在 master 节点请将 docker version 命令的返回结果提交到答题框。【1.5 分】

```
# 安装docker-ce
yum install -y docker-ce
# 启动Docker服务并设置为开机自启动：
systemctl start docker
systemctl enable docker
# 查看Docker版本信息：
docker version
```



# 2.安装docker应用 

所有节点配置阿里云镜像加速地址(https://d8b3zdiw.mirror.aliyuncs.com)并把启动引擎设置为 systemd，配置成功后加载配置文件并重启 docker 服务。

将 docker pull ubuntu 命令的返回结果提交到答题框。【1.5 分】 

````
# 编辑dockers配置文件
vi /etc/docker/daemon.json
```
{
  "registry-mirrors": ["https://d8b3zdiw.mirror.aliyuncs.com"],
   "exec-opts": ["native.cgroupdriver=systemd"]
}

```
# 加载配置文件并重启Docker服务：
systemctl daemon-reload
systemctl restart docker
````



# 3.载入镜像 

 在 master 节点/opt/images 目录下使用 tar 归档文件载入镜像。   将 docker images | grep mysql 命令的返回结果提交到答题框。【1.5 分】 

```
# 在 master 节点/opt/images 目录下使用 tar 归档文件载入镜像
cd /opt/images
docker load -i /opt/images/images.tar
# 查看Docker镜像中是否存在MySQL镜像：
docker images | grep mysql
```



# 4.安装docker-compose 

在 master 节点使用 /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64 文件安装 docker-compose。安装完成后执行 docker-compose version 命令。将 docker-compose version 命令的返回结果提交到答题框。【1.5 分】

```
# 赋予二进制文件运行权限
chmod +x /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64
# 创建软连接
ln -s /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64 /usr/local/bin/docker-compose
# 查看docker-compose的版本信息
docker-compose version
```



# 5.搭建horbor仓库 

在 master 节点解压/opt/harbor/ harbor-offline-installer-v2.5.3.tgz离线安装包，然后安装 harbor 仓库，并修改相应的 yml 文件，使各节点默认 docker 仓库为 harbor 仓库地址。 

在 master 节点请将 docker-compose ps 命令的返回结果提交到答题框。【1.5 分】

````
# 在 master 节点解压/opt/harbor/ harbor-offline-installer-v2.5.3.tgz离线安装包
tar -zxvf /opt/harbor/harbor-offline-installer-v2.5.3.tgz -C /opt/harbor/
# 安装Harbor仓库：
cd /opt/harbor/harbor
./install.sh
# 修改配置文件,将各节点的默认Docker仓库地址设置为Harbor仓库地址：
vi /opt/harbor/harbor/harbor.yml
```
hostname: harbor.example.com
harbor_admin_password: your_password
```
# 重新启动Harbor：
./install.sh
# 查看Docker Compose容器状态：
docker-compose -f /opt/harbor/harbor/docker-compose.yml ps
````



# 6.上传docker镜像 

在 master 节点执行/opt/k8s_image_push.sh 将所有镜像上传至 docker 仓库。 

将 docker login master 命令的返回结果提交到答题框(填写完整提示输入的内容)。【1.5 分】



# 7.部署Kubeadm、containerd、nerdctl和buildkit

 执行/opt/k8s_con_ner_bui_install.sh 部署 Kubeadm、containerd、nerdctl 和 buildkit。 

   将 ctr version 命令的返回结果提交到答题框。【1.5 分】 

```
# 在Master节点上运行脚本部署所需组件：
sh /opt/k8s_con_ner_bui_install.sh
ctr version
```



# 8.初始化集群 

在 master 节点kubeadm命令初始化集群，使用本地 Harbor 仓库。   将 kubectl get nodes 命令的返回结果提交到答题框。【1.5 分】 

```
sudo kubeadm init --image-repository=<harbor仓库 ip>
# master设置kubectl的配置：
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# 检查集群状态和节点：
kubectl get nodes
```



# 9.安装kubernetes网络插件 

修改提供的/opt/yaml/flannel/kube-flannel.yaml,使其镜像来源为本地 Harbor 仓库，然后安装 kubernetes 网络插件，安装完成后使用命令查看节点状态。

将 kubectl get pods -A 命令的返回结果提交到答题框。【1.5 分】

````
vi /opt/yaml/flannel/kube-flannel.yaml
```
containers:
- name: kube-flannel
  image: <harbor仓库 ip> /kube-flannel:v0.14.0
```
# master节点上运行安装网络插件：
kubectl apply -f /opt/yaml/flannel/kube-flannel.yaml
# 查看节点状态：
kubectl get pods -A
````

 

# 10.创建证书 

 给 kubernetes 创建证书,命名空间为 kubernetes-dashboard,涉及到的所有文件命名为 dashboard 例如 dashboard.crt。 

将 kubectl get csr 命令的返回结果提交到答题框。【1.5 分】 

```
# 创建命名空间
kubectl create namespace kubernetes-dashboard
# 创建证书Secret
kubectl create secret generic dashboard-certs --from-file=dashboard.crt --from-file=dashboard.key -n kubernetes-dashboard
# 查看证书签名请求（CSR）：
kubectl get csr
```



# 11.kubernetes 图形化界面的安装 

修改/opt/yaml/dashboard/recommended.yaml 的镜像来源为本地 Harbor 仓库，然后使用/opt/yaml/dashboa

rd/recommended.yaml 和/opt/yaml/dashboard/dashadmin-user.yaml安装 kubernetes dashboard 界面，完成后查看首页。 

将 kubectl get svc -n kubernetes-dashboard 命令的返回结果提交到答题框。【1.5 分】 

````
# 修改/opt/yaml/dashboard/recommended.yaml 的镜像来源为本地 Harbor 仓库
vi /opt/yaml/doshboard/recommended.yaml
```
image: <harbor仓库 ip>/kubernetesui/dashboard:v2.3.1
```
# 安装 kubernetes dashboard 界面
kubectl apply -f /opt/yaml/dashboard/recommended.yaml
kubectl apply -f /opt/yaml/dashboard/dashadmin-user.yaml
# 查看Dashboard首页：
kubectl get svc -n kubernetes-dashboard
````



# 12.删除污点 

为了能使 pod 调度到 master 节点,用命令删除污点。在浏览器访问 dashboard（https://IP:30001） 

将 kubectl describe nodes master | grep Taints 命令的返回结果提交到答题框。【1.5 分】 

```
# 删除污点
kubectl taint nodes master node-role.kubernetes.io/master-
# 访问Dashboard：
浏览器中访问https://<master ip>:30001
# 查看节点污点
kubectl describe nodes master | grep Taints
```



# 13.扩展计算节点 

在 node 节点执行 k8s_node_install.sh，将该节点加入 kubernetes 集群。完成后在 master 节点上查看所有节点状态。

在 master 节点请将 kubectl get nodes 命令的返回结果提交到答题框。【1.5 分】



# 任务3部署Owncloud网盘服务（20 分） 

ownCloud 是一个开源免费专业的私有云存储项目，它能帮你快速在个人电脑或服务器上架设一套专属

的私有云文件同步网盘，可以像 百度云那样实现文件跨平台同步、共享、版本控制、团队协作等。



# 1.创建PV和PVC 

 编写 yaml 文件(文件名自定义)创建 PV 和 PVC 来提供持久化存储，以便保存 ownCloud 服务中的文件和数据。 

要求：PV（访问模式为读写，只能被单个节点挂载;存储为 5Gi;存储类型为 hostPath,存储路径自定义）    PVC（访问模式为读写，只能被单个节点挂载;申请存储空间大小为 5Gi）    将 kubectl get pv,pvc 命令的返回结果提交到答题框。【4 分】 

````
# 编写yaml文件
vi pv.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: owncloud-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /your/custom/path # 请将此路径替换为实际路径

```
vi pvc.yaml
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: owncloud-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```
# 应用创建PV和PVC：
kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml
# 查看PV和PVC：
kubectl get pv,pvc
````



# 2. 配置ConfigMap

 编写 yaml 文件(文件名自定义)创建一个 configMap 对象，指定 OwnCloud 的环境变量。登录账号对应的环境变量为OWNCLOUD_ADMIN_USERNAME,密码对应的环境变量为OWNCLOUD_ADMIN_PASSWORD。

（变量值自定义）

  将 kubectl get ConfigMap 命令的返回结果提交到答题框。【4 分】 

````
vi configmap.yaml
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: owncloud-config
data:
  OWNCLOUD_ADMIN_USERNAME: your_admin_username
  OWNCLOUD_ADMIN_PASSWORD: your_admin_password
```
# 应用创建的ConfigMap：
kubectl apply -f configmap.yaml
# 查看ConfigMap：
kubectl get cm
````



# 3.创建Secret

编写 yaml 文件(文件名自定义)创建一个 Secret 对象，以保存 OwnCloud 数据库的密码。对原始密码采用 b

ase64 编码格式进行加密。  将 kubectl get Secret 命令的返回结果提交到答题框。【4 分】

````
vi secret.yaml
```
apiVersion: v1
kind: Secret
metadata:
  name: owncloud-secret
type: Opaque
data:
  DB_PASSWORD: encoded_base64_password

```
# 应用创建的Secret：
kubectl apply -f secret.yaml
# 查看Secret：
kubectl get Secret

````



# 4.部署owncloud Deployment应用 

编写 yaml 文件(文件名自定义) 创建 Deployment 对象, 指定 OwnCloud 的容器和相关的环境变量。(De ployment 资源命名为 owncloud-deployment,镜像为 Harbor 仓库中的 owncloud:latest，存储的挂载路径为/var/ www/html,其它根据具体情况进行配置)

将 kubectl describe pod 命令的返回结果提交到答题框。【4 分】 

````
vi owncloud-deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: owncloud-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: owncloud
  template:
    metadata:
      labels:
        app: owncloud
    spec:
      containers:
      - name: owncloud
        image: <harbor仓库 ip>/owncloud:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: owncloud-storage
          mountPath: /var/www/html
      volumes:
      - name: owncloud-storage
        persistentVolumeClaim:
          claimName: owncloud-pvc
      env:
      - name: OWNCLOUD_ADMIN_USERNAME
        valueFrom:
          configMapKeyRef:
            name: owncloud-config
            key: OWNCLOUD_ADMIN_USERNAME
      - name: OWNCLOUD_ADMIN_PASSWORD
        valueFrom:
          configMapKeyRef:
            name: owncloud-config
            key: OWNCLOUD_ADMIN_PASSWORD
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: owncloud-secret
            key: DB_PASSWORD
```
# 应用创建的Deployment：
kubectl apply -f owncloud-deployment.yaml
# 查看Pod描述：
kubectl describe pod -l app=owncloud
````



# 5.创建Service

编写 yaml 文件(文件名自定义)创建一个 Service 对象将 OwnCloud 公开到集群外部。通过 [http://IP](http://ip/)[:](http://ip/)端口号可查看 owncloud。【4 分】 

将 kubectl get svc -A 命令的返回结果提交到答题框。 

````
vi owncloud-service.yaml
```
apiVersion: v1
kind: Service
metadata:
  name: owncloud-service
spec:
  selector:
    app: owncloud
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: NodePort
```
# 应用创建的Service：
kubectl apply -f owncloud-service.yam
````



C模块题目：企业级应用的自动化部署和运维

虚拟机与环境规划 

 

表3 

| 设备名称  | 主机名  | 接口 | IP地址                                   | 角色 |
| --------- | ------- | ---- | ---------------------------------------- | ---- |
| 云服务器1 | ansible | eth0 | 公网 IP:******** 私网IP:192.168.100.*/24 |      |
| 云服务器2 | host1   | eth0 | 公网 IP:******** 私网IP:192.168.100.*/24 |      |
| 云服务器3 | host2   | eth0 | 公网 IP:******** 私网IP:192.168.100.*/24 |      |

 

\1. 上表中的公网IP以自己云主机显示为准，每个人的公网IP不同。使用第三方软件远程连接云主机，使用公网IP连接。 

2.华为云中云主机名字已命好，直接使用对应名字的云主机即可。 

 

任务 1 企业级应用的自动化部署（15分） 

\1. Ansible 自动化运维工具部署主从数据库 

（1）    修改主机名 ansible 节点主机名为 ansible,host1 节点主机名为 host1,host2 节点主机名为 host2,请使用提供的软件包在 ansible 节点安装 ansible。

将 ansible --version 命令的返回结果提交到答题框。 

```
# 在ansible节点上执行：
hostname set-hostname ansible
# 在host1节点上执行：
hostname set-hostname host1
# 在host2节点上执行：
hostnamectl set-hostname host2
# 在ansible节点上安装Ansible：
yum install /path/to/ansible.rpm
```



（2）    配置主机清单文件，创建 mysql 主机组，mysql 主机组内添加 host1 和 host2 主机；创建 mysql1 主机组，mysql1 组内添加 host1 主机；创建 mysql2 主机组，mysql2 组内添加 host2 主机，并配置免密登录。 

将 ansible all -m ping 命令的返回结果提交到答题框。 

````
# 在ansible节点上
vi /etc/ansible/hosts
```
[mysql]
<host1 ip> host1
<host2 ip>host2

[mysql1]
<host1> host1

[mysql2]
<host2> host2
```
# 运行ansible命令进行主机连接测试：
ansible all -m ping
````



（3）mysql 主机组内所有主机安装 mariadb 数据库，启动数据库并设置为开机自启动。

在 host1 节点将 systemctl status mariadb 命令的返回结果提交到答题框。

````
# 安装MariaDB并启动：
vi install_mariadb.yml
```
---
- name: Install and start MariaDB
  hosts: mysql
  become: yes
  tasks:
    - name: Install MariaDB
      yum:
        name: mariadb-server
        state: present

    - name: Start and enable MariaDB
      systemd:
        name: mariadb
        state: started
        enabled: yes

```
# 运行Playbook来执行安装和启动操作：
ansible-playbook install_mariadb.yml
# 检查MariaDB状态:
systemctl status mariadb
````



（4）    编写一名称为 mariadb.sh 的 shell 脚本，该脚本具有完成 mariadb 数据库的初始化功能(要求数据库用户名为 root,密码为 123456)，通过 ansible 对应模块执行 mariadb.sh 完成对 mysql 主机组下的所有节点进行数据库初始化。 

在 node1 节点，将 mysql -uroot -p123456 命令的返回结果提交到答题框。 

````
vi init_mariadb.yml
```
---
- name: Initialize MariaDB
  hosts: mysql
  become: yes
  tasks:
    - name: Copy mariadb.sh script
      copy:
        src: /path/to/mariadb.sh
        dest: /tmp/mariadb.sh
        mode: 0755

    - name: Execute mariadb.sh script
      shell: /tmp/mariadb.sh

```
# 运行Playbook来执行脚本在mysql主机组内的所有节点上初始化数据库：
ansible-playbook init_mariadb.yml
# 连接到MariaDB数据库：
mysql -uroot -p123456
````



（5）    创建主机变量，所有主机组中 host1 节点创建变量 id=20,hots2 节点创建变量 id=30。 

将 cat /etc/ansible/hosts | grep id 命令的返回结果提交到答题框。

```
# 编辑主机变量：
vi /etc/ansible/hosts
[mysql]
host1 id=20
host2 id=30

[mysql1]
host1 id=20

[mysql2]
host2 id=30
# 检查主机变量设置：
cat /etc/ansible/hosts | grep id
```



（6）    根据 mysql 配置文件创建 mysql 的 Janja2 模板文件命名为 my.cnf.j2,编写 mariadb.yaml 文件实现主从数据库的配置和权限设置。 

在 host2 节点进入数据库将 show slave status \G 命令的返回结果提交到答题框。 

````
# 创建一个名为my.cnf.j2的文件
vi my.cnf.j2
```
[mysqld]
server-id={{ server_id }}
log-bin=mysql-bin
binlog-do-db=mydb

```
# 编写mariadb.yaml配置文件：
vi mariadb.yaml
```
---
- name: Configure MySQL replication and privileges
  hosts: host1,host2
  become: yes
  vars:
    server_id: 1  # Set appropriate server ID for each host
  tasks:
    - name: Copy my.cnf.j2 template
      template:
        src: /path/to/my.cnf.j2
        dest: /etc/my.cnf
      notify:
        - restart mysql

    - name: Start MySQL service
      service:
        name: mariadb
        state: started

    - name: Set MySQL root password
      mysql_user:
        name: root
        password: 123456
        login_unix_socket: /var/lib/mysql/mysql.sock

    # Additional tasks for setting up replication
    # ...

  handlers:
    - name: restart mysql
      service:
        name: mariadb
        state: restarted

```
# 运行配置Playbook
ansible-playbook mariadb.yaml
# 连接到数据库并查看从数据库状态：
mysql -uroot -p123456 -e "show slave status \G"
````



 

2.Mycat 读写分离部署 

(1)在 ansible 节点，安装 Java 的运行环境。 将 java –version 命令的返回结果提交到答题框。

```
# 安装java的运行环境
yum install java-1.8.0-openjdk
# 检查Java安装结果：
java -version
```



(2)   在主机清单创建 ansible 主机组并添加 ansible 节点。把 Mycat-server-1.6-RELEASE-linux.tar.gz 解压到/usr/local 目录下，并完成相关配置。将 cat /etc/profile | grep mycat 命令的返回结果提交到到答题框。 

````
# 编辑主机清单
vi /etc/ansible/hosts
```
[ansible]
<ansible ip> ansible
```
# 把 Mycat-server-1.6-RELEASE-linux.tar.gz 解压到/usr/local 目录下
tar -zxvf Mycat-server-1.6-RELEASE-linux.tar.gz -C /usr/local
# 进入Mycat目录
cd /usr/local/Mycat-server-1.6-RELEASE
# 编辑Mycat配置文件，根据需要进行配置
vi conf/server.xml
# 更新环境变量：
source /etc/profile
````





(3)   编辑 mycat 服务读写分离的 schema.xml 配置文件，配置数据库 host1 为写节点，host2 为读节点。编辑 m ycat 服务的 sever.xml 配置文件，配置 mycat 的逻辑库以及 root 用户的密码为 123456。通过 mycat 启动脚本文件启动 mycat 并验证 mycat 监听端口号是否被监听。将 netstat –ntpl 命令的返回结果提交到答题框。

````
vi conf/schema.xml 
```
<dataHost name="host1" ... writeType="true">
    <!-- 写节点配置 -->
    ...
</dataHost>

<dataHost name="host2" ... writeType="false">
    <!-- 读节点配置 -->
    ...
</dataHost>

```
vi server.xml
```
<user name="root" defaultAccount="true">
    <property name="password">123456</property>
</user>

```
# 启动Mycat并验证监听端口：
  # 进入Mycat目录
  cd /usr/local/Mycat-server-1.6-RELEASE

  # 启动Mycat
  ./bin/mycat start

  # 查看监听端口情况
  netstat -ntpl | grep java

````

 

3. Ansible 自动化运维工具部署 zookeeper 集群

zookeeper 是一个分布式服务框架，是 Apache Hadoop 的一个子项目，主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 gpmall 商城系统中用到了 kafka 消息队列，kafka 集群的搭建依赖 zookeeper 集群来进行元数据的管理。

（1）    编写主机清单文件，创建 zookeeper 主机组，zookeeper 主机组内添加 ansible、host1 和 host2 主机，分别创建主机变量 zk_id=私有 IP 最后一个数字。

````
vi /etc/ansible/hosts
```
[zookeeper]
ansible zk_id=1
host1 zk_id=2
host2 zk_id=3
```
# 编写ansible playbook：
vi deploy_zookeeper.yml
```
---
- name: Deploy ZooKeeper Cluster
  hosts: zookeeper
  become: yes
  tasks:
    - name: Install Java
      yum:
        name: java-1.8.0-openjdk
        state: present

    - name: Create ZooKeeper data directory
      file:
        path: /opt/zookeeper/data
        state: directory
        owner: zookeeper
        group: zookeeper

    - name: Download and extract ZooKeeper
      unarchive:
        src: http://apache.mirrors.pair.com/zookeeper/zookeeper-3.6.3/apache-zookeeper-3.6.3-bin.tar.gz
        dest: /opt
        remote_src: yes

    - name: Configure ZooKeeper
      template:
        src: zoo.cfg.j2
        dest: /opt/apache-zookeeper-3.6.3-bin/conf/zoo.cfg

    - name: Start ZooKeeper service
      systemd:
        name: zookeeper
        enabled: yes
        state: started

```
# 在上面的Playbook中，您需要将zoo.cfg.j2替换为ZooKeeper配置文件的Jinja2模板。
# 运行Playbook：
ansible-playbook deploy_zookeeper.yml

````



将 ansible all -a "id"命令的返回结果提交到答题框。

（2）    在 ansible 节点，使用提供的 zookeeper-3.4.14.tar.gz 软件包，编写 zookeeper.yaml 文件,实现 zookeeper 集群搭建，创建任务清单实现 zookeeper 安装包批量解压、通过 Janja2 模板文件配置 zookeeper、创建 zooke eper 的 myid 文件和批量启动 zookeeper 功能。在三个节点相应的目录使用./zkServer.sh status 命令查看三个 Zookeeper 节点的状态。将 jps 命令的返回结果提交到答题框。 

vi zookeeper.yaml
````
---
- name: Deploy ZooKeeper Cluster
  hosts: zookeeper
  become: yes
  tasks:
    - name: Copy ZooKeeper installation files
      copy:
        src: /path/to/zookeeper-3.4.14.tar.gz
        dest: /tmp

    - name: Extract ZooKeeper
      unarchive:
        src: /tmp/zookeeper-3.4.14.tar.gz
        dest: /opt
        creates: /opt/zookeeper-3.4.14

    - name: Configure ZooKeeper
      template:
        src: zoo.cfg.j2
        dest:
创建Jinja2模板
vi zoo.cfg.j2
```
tickTime=2000
dataDir=/opt/zookeeper-3.4.14/data
clientPort=2181

{% for host in groups['zookeeper'] %}
server.{{ hostvars[host]['ansible_zk_id'] }}={{ host }}:2888:3888
{% endfor %}
```
# 运行Playbook：
ansible-playbook zookeeper.yaml
# 查看ZooKeeper节点的状态：
ansible-playbook zookeeper.yaml --tags "Check ZooKeeper Cluster Status"
````
4. Ansible 自动化运维工具部署 kafka 集群

Apache Kafka 是分布式发布-订阅消息系统，是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。gpmall 商城系统使用了 kafka 消息队列框架来缓存消息。在 ansible 节点，使用提供的 kafka_2.11-1.1.1.tgz 软件包，编写 ansible-playbook 文件实现 kafka 集群搭建，创建任务清单实现 kaf ka 安装包批量解压、通过 Janja2 模板文件批量配置 kafka 和批量启动 kafka 功能。将 jps –ml 命令的返回结果提交到答题框。 

````
vi kafka.yaml
```
---
- name: Deploy and Start Kafka Cluster
  hosts: kafka
  become: yes
  tasks:
    - name: Copy Kafka installation files
      copy:
        src: /path/to/kafka_2.11-1.1.1.tgz
        dest: /tmp

    - name: Extract Kafka
      unarchive:
        src: /tmp/kafka_2.11-1.1.1.tgz
        dest: /opt
        creates: /opt/kafka_2.11-1.1.1

    - name: Configure Kafka
      template:
        src: server.properties.j2
        dest: /opt/kafka_2.11-1.1.1/config/server.properties

    - name: Start Kafka service
      shell: /opt/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /opt/kafka_2.11-1.1.1/config/server.properties

- name: Check Kafka Cluster Status
  hosts: kafka
  tasks:
    - name: Check Kafka status
      shell: jps -ml | grep kafka.Kafka

```
# 创建Jinja2模板（例如server.properties.j2）：
vi server.properties.j2
```
broker.id={{ ansible_play_hosts.index(inventory_hostname) }}
listeners=PLAINTEXT://{{ ansible_host }}:9092
advertised.listeners=PLAINTEXT://{{ ansible_host }}:9092
log.dirs=/opt/kafka_2.11-1.1.1/logs
```
# 运行Playbook：
ansible-playbook -i hosts.ini kafka.yaml
# 使用以下命令查看Kafka节点的状态：
ansible-playbook kafka.yaml --tags "Check Kafka Cluster Status"
````



任务2 应用商城系统部署

1.在 ansible 节点，使用提供的 gpmall-cluster 软件包，完成集群应用系统部署。部署完成后，进行登录，最后使用 curl 命令去获取商城首页的返回信息， 

将 curl -l [http://EIP](http://eip/)[:80](http://eip/)[命](http://eip/)令的返回结果提交到答题框。

````
vi gpmall.yaml
```
---
- name: Deploy gpmall Cluster
  hosts: ansible  # Use the appropriate host group or hostname
  become: yes
  tasks:
    - name: Copy gpmall-cluster software package
      copy:
        src: /path/to/gpmall-cluster
        dest: /tmp

    - name: Extract gpmall-cluster
      unarchive:
        src: /tmp/gpmall-cluster
        dest: /opt
        creates: /opt/gpmall-cluster

    - name: Start gpmall-cluster services
      shell: /opt/gpmall-cluster/start.sh

- name: Get gpmall Homepage
  hosts: ansible  # Use the appropriate host group or hostname
  tasks:
    - name: Get gpmall homepage
      shell: curl -s http://EIP[:80]
      register: homepage_response

    - debug:
        var: homepage_response.stdout

```
# 运行Playbook：
ansible-playbook gpmall.yaml

````

 

任务3 Prometheus监控Mariadb主从数据库

1.Prometheus 及 Grafana 搭建

根据 grafana-enterprise-8.3.6.linux-amd64.tar.gz、prometheus-2.37.0.linux-amd64.tar.gz、node_exporter-1.3.

1.linux-amd64.tar.gz 资源包，安装 prometheus-2.37.0、node_exporter 服务并启动，安装 grafana 服务并测试浏览器登陆。

 ```
 # 安装和配置Prometheus：
 tar -zxvf prometheus-2.37.0.linux-amd64.tar.gz
 mv prometheus-2.37.0.linux-amd64 prometheus
 cd prometheus
 ./prometheus --config.file=prometheus.yml
 # 这将在默认端口9090上启动Prometheus。
 # 安装和启动Node Exporter：
 tar -xzvf node_exporter-1.3.1.linux-amd64.tar.gz
 cd node_exporter-1.3.1.linux-amd64
 ./node_exporter
 # Node Exporter将在默认端口9100上启动。
 # 安装和配置Grafana：
 tar -xzvf grafana-enterprise-8.3.6.linux-amd64.tar.gz
 mv grafana-8.3.6 grafana
 cd grafana/bin
 ./grafana-server
 # Grafana将在3000端口上启动。您可以在浏览器中访问http://服务器IP:3000来登录Grafana，默认用户名和密码都是admin。
 # 配置数据源和Dashboard：
   登录Grafana后，点击左侧的“齿轮”图标，选择“Data Sources”。
   点击“Add data source”。
   选择“Prometheus”作为数据源类型，配置URL为http://localhost:9090（如果Prometheus在其他服务器上，请   替换为相应的URL）。
   点击“Save & Test”以保存和测试数据源配置。
 # 导入Mariadb监控Dashboard：
   在Grafana中，点击左侧的“+”图标，选择“Import”。
   使用Dashboard ID（如https://grafana.com/grafana/dashboards/7362）或者上传JSON文件导入Mariadb   监控Dashboard。
   
 ```



2.Prometheus 配置自定义监控项、mysqld_exporter 安装及配置

根据 mysqld_exporter-0.12.1.linux-amd64.tar.gz 安装 mysqld_exporter，并创建/修改 mysqld_exporter-0.12.

1.linux-amd64/my.cnf 文件添加被监控数据库用户名密码，修改 prometheus.yml 文件，添加两个任务：mysql -ansible-slave、nodes，抓取间隔都设置 5s，分别监控主从节点的 mysqld_exporter、node_exporter 端口，重启 prometheus 并刷新 prometheus 页面。

````
# 安装和配置 mysqld_exporter：
tar -xzvf mysqld_exporter-0.12.1.linux-amd64.tar.gz
cd mysqld_exporter-0.12.1.linux-amd64
# 编辑my.cnf文件，添加被监控数据库用户名密码
cp my.cnf.example my.cnf
vi my.cnf
# 启动 mysqld_exporter
./mysqld_exporter
# 修改 Prometheus 配置：添加以下内容以监控mysqld_exporter和node_exporter：
vi prometheus.yml
```
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'mysql-ansible-slave'
    static_configs:
      - targets: ['host1:9104', 'host2:9104'] # 替换为实际的主从节点IP

  - job_name: 'nodes'
    static_configs:
      - targets: ['host1:9100', 'host2:9100'] # 替换为实际的主从节点IP

```
````



 

 